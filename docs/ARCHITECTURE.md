# Video Analysis AI - Software Architecture# Architecture Deep Dive



> **Last Updated:** October 19, 2025## System Overview



## ðŸ“‹ Table of ContentsThe Video Analysis AI is a fully local, offline-capable desktop application that uses a multi-agent architecture coordinated through the Model Context Protocol (MCP).

- [High-Level Architecture](#high-level-architecture)

- [Query Processing Flow](#query-processing-flow)## Components

- [Component Details](#component-details)

- [Communication Protocols](#communication-protocols)### 1. Frontend (React + Tauri)

- [Data Structures](#data-structures)

- [Cache Strategy](#cache-strategy)**Technology**: React 18, TypeScript, Tailwind CSS, Tauri 1.5



---**Responsibilities**:

- User interface and interaction

## High-Level Architecture- Video file upload

- Chat-based query interface

```- Display analysis results

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Download generated reports

â”‚   Frontend (Tauri Desktop App)          â”‚

â”‚   React + TypeScript                    â”‚**Key Files**:

â”‚   â”œâ”€ Chat Interface                     â”‚- `App.tsx` - Main application component

â”‚   â”œâ”€ Video Upload                       â”‚- `ChatInterface.tsx` - Chat UI with streaming support

â”‚   â””â”€ Report Viewer                      â”‚- `VideoUpload.tsx` - Drag-and-drop video upload

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- `grpcClient.ts` - gRPC communication layer

             â”‚ gRPC (Binary Protocol)

             â”‚ Port 50051**Communication**: 

             â–¼- Uses gRPC to communicate with backend

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Tauri provides native desktop capabilities

â”‚   Backend (Python gRPC Server)          â”‚- Streaming responses for real-time feedback

â”‚   â”œâ”€ Session Manager (File-based)       â”‚

â”‚   â”œâ”€ Video Store (In-memory + Cache)    â”‚### 2. Backend (Python)

â”‚   â”œâ”€ MCP Orchestrator (Intent-driven)   â”‚

â”‚   â””â”€ gRPC Service Layer                 â”‚**Technology**: Python 3.11+, FastAPI, gRPC, Ollama Client

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             â”‚**Responsibilities**:

      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”- gRPC server hosting

      â”‚             â”‚- Request orchestration

      â–¼             â–¼- MCP client management

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Video file storage and metadata

â”‚  Ollama   â”‚  â”‚  MCP Agents      â”‚- Ollama LLM integration for planning

â”‚  (Local)  â”‚  â”‚  (Stdio Spawn)   â”‚

â”‚           â”‚  â”‚                  â”‚**Key Modules**:

â”‚ llama3.2  â”‚  â”‚ â”œâ”€ Transcription â”‚

â”‚ llava     â”‚  â”‚ â”œâ”€ Vision        â”‚#### `grpc_server/`

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â””â”€ Report        â”‚- `server.py` - gRPC server initialization

               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- `service.py` - Service method implementations

```

#### `mcp/`

---- `client.py` - MCP client for calling agent tools

- `orchestrator.py` - Agent coordination logic

## Query Processing Flow

#### `models/`

### Intent-Driven Cache-First Architecture- `video_store.py` - In-memory video metadata store



```#### `utils/`

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- `config.py` - Configuration management

â”‚   User Query     â”‚- `setup.py` - Dependency checks and initialization

â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚**Communication**:

         â–¼- Exposes gRPC API to frontend

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Makes HTTP calls to MCP servers

â”‚ [1] ANALYZE INTENT                  â”‚- Uses Ollama API for LLM inference

â”‚  - Classify query type              â”‚

â”‚  - Determine required data          â”‚### 3. Ollama (LLM Layer)

â”‚  - Identify output format           â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜**Technology**: Ollama, LLaMA 3.2, LLaVA

         â”‚

         â–¼**Responsibilities**:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Query understanding and planning

â”‚ [2] CHECK CACHE                     â”‚- Agent selection and coordination

â”‚  - Session folder lookup            â”‚- Response synthesis

â”‚  - data/sessions/{id}/cache/*.json  â”‚- Vision analysis (via LLaVA)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚**Models Used**:

         â–¼- `llama3.2` - Main reasoning and planning model

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”- `llava` - Vision analysis model

    â”‚Missing?â”‚

    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜**Flow**:

        â”‚1. User query â†’ Ollama analyzes intent

   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”2. Ollama creates execution plan (JSON)

   â”‚         â”‚3. Plan specifies which agents to invoke

  Yes       No4. Results â†’ Ollama synthesizes final answer

   â”‚         â”‚

   â–¼         â–¼### 4. MCP Servers (Agents)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ [3] PLAN TOOLS   â”‚    â”‚ [5] SYNTHESIZE   â”‚#### Transcription Agent

â”‚  - Which agents? â”‚    â”‚  - Use cached    â”‚

â”‚  - Which tools?  â”‚    â”‚    data          â”‚**Technology**: Python, MCP SDK, FFmpeg, (Whisper.cpp)

â”‚  - Execution     â”‚    â”‚  - Generate      â”‚

â”‚    order         â”‚    â”‚    response      â”‚**Tools**:

â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- `extract_audio` - Extract audio from video using FFmpeg

    â”‚                       â”‚- `transcribe_audio` - Convert speech to text

    â–¼                       â”‚- `get_timestamps` - Get timestamped segments

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚

â”‚ [4] EXECUTE      â”‚        â”‚**Integration Points**:

â”‚  - Spawn agents  â”‚        â”‚- FFmpeg for audio extraction

â”‚  - Run tools     â”‚        â”‚- Placeholder for Whisper.cpp integration

â”‚  - Cache results â”‚        â”‚- Returns structured transcription data

â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚

    â”‚                       â”‚#### Vision Agent

    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â”‚**Technology**: Python, MCP SDK, OpenCV, (Tesseract, LLaVA)

                â–¼

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”**Tools**:

        â”‚ [6] OUTPUT &  â”‚- `extract_frames` - Extract key frames at intervals

        â”‚     CACHE     â”‚- `analyze_frame` - Describe frame content

        â”‚  - Stream to  â”‚- `detect_objects` - Object detection (placeholder)

        â”‚    frontend   â”‚- `extract_text` - OCR text extraction

        â”‚  - Save cache â”‚- `analyze_chart` - Chart and graph analysis (placeholder)

        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```**Integration Points**:

- OpenCV for frame extraction

### Detailed Flow Example- Tesseract for OCR

- Placeholder for vision models

**User:** "Summarize this video"

#### Report Agent

```

1. ANALYZE INTENT**Technology**: Python, MCP SDK, ReportLab, python-pptx

   â”œâ”€ Intent: "summarize"

   â”œâ”€ Output: "pdf"**Tools**:

   â””â”€ Required Data: - `create_pdf_report` - Generate PDF document

       â”œâ”€ transcription- `create_ppt_report` - Generate PowerPoint presentation

       â”œâ”€ frames- `format_content` - Structure analysis results

       â”œâ”€ visual_analysis

       â””â”€ summary**Integration Points**:

- ReportLab for PDF generation

2. CHECK CACHE- python-pptx for PowerPoint creation

   â”œâ”€ Check: data/sessions/{id}/cache/transcription.json- Template-based formatting

   â”œâ”€ Check: data/sessions/{id}/cache/frames.json

   â”œâ”€ Check: data/sessions/{id}/cache/visual_analysis.json## Data Flow

   â””â”€ Check: data/sessions/{id}/cache/summary.json

   ### Video Upload Flow

   Result: âŒ transcription missing

           âŒ visual_analysis missing```

           âœ… frames cached1. User drags video file

   â†“

3. PLAN TOOLS2. Frontend reads file as chunks

   â”œâ”€ Agent: transcription   â†“

   â”‚   â”œâ”€ extract_audio3. gRPC stream to backend

   â”‚   â””â”€ transcribe_audio   â†“

   â”œâ”€ Agent: vision4. Backend saves to uploads/

   â”‚   â””â”€ analyze_frame (use cached frames)   â†“

   â””â”€ Skip: frames (already cached)5. Extract metadata (OpenCV)

   â†“

4. EXECUTE AGENTS6. Store in video_store

   â”œâ”€ Spawn: transcription-agent via stdio   â†“

   â”‚   â””â”€ Cache: transcription.json7. Return video_id to frontend

   â”œâ”€ Spawn: vision-agent via stdio```

   â”‚   â””â”€ Cache: visual_analysis.json

   â””â”€ All results collected### Query Flow



5. SYNTHESIZE```

   â”œâ”€ Aggregate all data1. User types question

   â”œâ”€ Generate structured summary with Ollama   â†“

   â””â”€ Format for report2. Frontend sends ChatRequest

   â†“

6. OUTPUT & CACHE3. Backend receives query

   â”œâ”€ Generate PDF report   â†“

   â”œâ”€ Cache: summary.json4. Ollama plans execution

   â””â”€ Return: PDF path to frontend   â†“

```5. Backend calls MCP agents

   â†“

---6. Agents execute tools

   â†“

## Component Details7. Results collected

   â†“

### 1. Frontend (Tauri + React)8. Ollama synthesizes answer

   â†“

**Tech Stack:**9. Stream response to frontend

- **Framework:** React 18 + TypeScript```

- **Desktop:** Tauri 1.5 (Rust)

- **Styling:** Tailwind CSS### Agent Execution Flow

- **Communication:** gRPC (tonic + prost)

```

**Key Responsibilities:**1. Orchestrator receives plan

- User interface and interactions   â†“

- Video file upload (chunked streaming)2. For each agent in plan:

- Real-time chat with AI   â”‚

- Report viewing and download   â”œâ”€> MCP Client calls agent tool

- Session management   â”‚   â†“

   â”‚   Agent processes request

**File Structure:**   â”‚   â†“

```   â”‚   Agent returns result

frontend/   â”‚   â†“

â”œâ”€â”€ src/   â””â”€> Result added to collection

â”‚   â”œâ”€â”€ App.tsx                 # Main component   â†“

â”‚   â”œâ”€â”€ components/3. All results aggregated

â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx   # Chat UI   â†“

â”‚   â”‚   â””â”€â”€ VideoUpload.tsx     # Upload UI4. Return to orchestrator

â”‚   â””â”€â”€ services/```

â”‚       â””â”€â”€ grpc.ts             # gRPC client

â””â”€â”€ src-tauri/## Communication Protocols

    â”œâ”€â”€ src/

    â”‚   â””â”€â”€ main.rs             # Rust gRPC client### gRPC (Frontend â†” Backend)

    â””â”€â”€ Cargo.toml              # Dependencies

```**Protocol**: HTTP/2-based RPC



**gRPC Client (Rust):****Messages**:

```rust- `ChatRequest` / `ChatResponse` - Chat messages

// Native gRPC client using tonic- `VideoChunk` / `VideoUploadResponse` - File upload

pub async fn chat(request: ChatRequest) -> Result<ChatResponse> {- `VideoQuery` / `QueryResponse` - Video queries

    let mut client = VideoAnalysisClient::connect("http://localhost:50051").await?;- `ReportRequest` / `ReportResponse` - Report generation

    let response = client.chat(request).await?;- `StatusRequest` / `AnalysisStatus` - Progress tracking

    Ok(response.into_inner())

}**Features**:

```- Bidirectional streaming

- Protocol buffer serialization

---- Type safety

- High performance

### 2. Backend (Python gRPC Server)

### MCP (Backend â†” Agents)

**Tech Stack:**

- **Language:** Python 3.11+**Protocol**: Model Context Protocol over HTTP

- **RPC:** gRPC (grpcio)

- **LLM:** Ollama API client**Structure**:

- **Agents:** MCP SDK 0.9.1 (stdio)```json

{

**Key Responsibilities:**  "method": "tools/call",

- gRPC API endpoints  "params": {

- Session and file management    "name": "tool_name",

- MCP orchestration (intent-driven)    "arguments": { ... }

- Video metadata extraction  }

- Cache management}

```

**File Structure:**

```**Tools Discovery**:

backend/```json

â”œâ”€â”€ main.py                     # Entry point{

â”œâ”€â”€ src/  "method": "tools/list",

â”‚   â”œâ”€â”€ grpc_server/  "params": {}

â”‚   â”‚   â”œâ”€â”€ server.py           # gRPC server setup}

â”‚   â”‚   â”œâ”€â”€ service.py          # Service methods```

â”‚   â”‚   â””â”€â”€ video_analysis_pb2.py  # Generated proto

â”‚   â”œâ”€â”€ mcp/**Features**:

â”‚   â”‚   â”œâ”€â”€ client.py           # MCP agent client- Standardized agent communication

â”‚   â”‚   â””â”€â”€ orchestrator.py     # Intent-driven orchestrator- Tool discovery

â”‚   â”œâ”€â”€ models/- JSON-based messages

â”‚   â”‚   â””â”€â”€ video_store.py      # Video metadata store- Extensible

â”‚   â””â”€â”€ utils/

â”‚       â”œâ”€â”€ config.py           # Configuration### Ollama API (Backend â†” LLM)

â”‚       â””â”€â”€ session_manager.py  # Session handling

â””â”€â”€ requirements.txt**Protocol**: HTTP REST

```

**Endpoints**:

**Session Structure:**- `POST /api/chat` - Chat completion

```- `POST /api/generate` - Text generation

data/sessions/{session_id}/- `GET /api/tags` - List models

â”œâ”€â”€ session.json            # Session metadata

â”œâ”€â”€ messages.json           # Chat history**Features**:

â”œâ”€â”€ uploads/                # Video files- Local inference

â”‚   â””â”€â”€ {video_id}_{filename}- No external API calls

â”œâ”€â”€ cache/                  # Analysis cache- Model management

â”‚   â”œâ”€â”€ transcription.json- Streaming responses

â”‚   â”œâ”€â”€ frames.json

â”‚   â”œâ”€â”€ visual_analysis.json## Security Considerations

â”‚   â””â”€â”€ summary.json

â”œâ”€â”€ temp/                   # Temporary files### Local-First Architecture

â”‚   â””â”€â”€ {video_id}_audio.wav

â””â”€â”€ reports/                # Generated reports- All data stays on device

    â””â”€â”€ {title}.pdf- No cloud API calls

```- No telemetry or tracking

- User data never leaves machine

---

### File Handling

### 3. MCP Orchestrator (Intent-Driven)

- Videos stored in configured directory

**Core Logic:**- Temporary files cleaned up

- File size limits enforced

```python- Path traversal protection

class MCPOrchestratorV2:

    async def process_query(self, query: str, video_id: str, session: Session):### Desktop Security

        # Step 1: Analyze intent

        intent = await self.analyze_intent(query)- Tauri provides sandboxing

        # Returns: {- No web-based vulnerabilities

        #   "intent": "summarize",- Native OS security features

        #   "output_type": "pdf", - Code signing in production

        #   "required_data": ["transcription", "visual_analysis"]

        # }## Scalability

        

        # Step 2: Check cache### Current Limitations

        cache_status = self.check_cache(intent["required_data"], session.cache)

        # Returns: {- Single-threaded video processing

        #   "transcription": False,  # Missing- In-memory video store

        #   "visual_analysis": False # Missing- Sequential agent execution

        # }- Limited by local hardware

        

        # Step 3: Plan tools### Future Optimizations

        tools_plan = self.plan_missing_tools(intent["required_data"], cache_status)

        # Returns: [- Parallel agent execution

        #   {"agent": "transcription", "tool": "extract_audio"},- Persistent video database

        #   {"agent": "transcription", "tool": "transcribe_audio"},- GPU acceleration

        #   {"agent": "vision", "tool": "analyze_frame"}- Batch processing

        # ]- Result caching

        

        # Step 4: Execute tools## Extension Points

        results = await self.execute_tools(tools_plan, video_id, session.cache)

        # Spawns agents, runs tools, caches results### Adding New Agents

        

        # Step 5: Synthesize1. Create new MCP server

        response = await self.synthesize_response(intent, results)2. Define tools in MCP format

        3. Register in orchestrator

        # Step 6: Cache and return4. Update system prompt

        session.cache.set("summary", response)5. Configure in `.env`

        return response

```### Adding New LLM Models



**Intent Analysis (Ollama):**1. Pull model via Ollama

```python2. Update configuration

prompt = f"""Analyze this user query: "{query}"3. Adjust prompts if needed

4. Test compatibility

Return JSON:

{{### Custom Report Templates

  "intent": "summarize | transcribe | visual_analysis | chat",

  "output_type": "text | pdf | ppt",1. Extend report agent

  "required_data": ["transcription", "frames", "visual_analysis"]2. Add template system

}}3. Expose via tool parameters

"""4. Update frontend UI

response = ollama.chat(model="llama3.2", messages=[...])

```## Performance Characteristics



---### Video Upload

- Speed: ~10-50 MB/s

### 4. MCP Agents (Stdio Spawn)- Limited by: Disk I/O



**Architecture:** Agents spawn on-demand via stdio (not HTTP servers)### Transcription

- Speed: ~1-5x realtime

#### Transcription Agent- Limited by: Model size, CPU/GPU

```

Tools:### Frame Analysis

â”œâ”€ extract_audio(video_path, output_path)- Speed: ~0.5-2 frames/sec

â”‚   â””â”€ Uses: FFmpeg- Limited by: Vision model, GPU

â”œâ”€ transcribe_audio(audio_path)

â”‚   â””â”€ Uses: Whisper (future)### Report Generation

â””â”€ Returns: {text, timestamps}- Speed: <1 second

```- Limited by: Content complexity



#### Vision Agent## Monitoring and Debugging

```

Tools:### Logs

â”œâ”€ extract_frames(video_path, interval, output_dir)

â”‚   â””â”€ Uses: OpenCV- Backend: `backend/app.log`

â”œâ”€ analyze_frame(frame_path)- MCP Servers: stdout/stderr

â”‚   â””â”€ Uses: Ollama LLaVA- Frontend: Developer console

â”œâ”€ detect_objects(frame_path)

â”‚   â””â”€ Uses: YOLO (future)### Health Checks

â””â”€ extract_text(frame_path)

    â””â”€ Uses: Tesseract OCR- Ollama connection check on startup

```- MCP server availability check

- gRPC connection status

#### Report Agent

```### Error Handling

Tools:

â”œâ”€ create_pdf_report(content, output_path)- Graceful degradation

â”‚   â””â”€ Uses: ReportLab- User-friendly error messages

â”œâ”€ create_ppt_report(content, output_path)- Detailed logging for debugging

â”‚   â””â”€ Uses: python-pptx- Retry logic for transient failures

â””â”€ Returns: {path, size_bytes}
```

**Spawn Flow:**
```python
# Backend spawns agent on-demand
process = subprocess.Popen(
    ["python", "mcp-servers/vision-agent/server.py"],
    stdin=subprocess.PIPE,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)

# Send MCP request via stdin
request = {"method": "tools/call", "params": {...}}
process.stdin.write(json.dumps(request).encode())

# Read response from stdout
response = json.loads(process.stdout.readline())
```

---

## Communication Protocols

### gRPC (Frontend â†” Backend)

**Protocol:** HTTP/2 Binary RPC  
**Port:** 50051  
**Format:** Protocol Buffers

**Key Messages:**
```protobuf
// Chat
message ChatRequest {
    string user_message = 1;
    string session_id = 2;
    optional string video_id = 3;
}

message ChatResponse {
    string ai_response = 1;
    bool success = 2;
    optional string error = 3;
}

// Video Upload
message VideoChunk {
    bytes chunk = 1;
    string filename = 2;
    int32 chunk_index = 3;
    optional VideoMetadata metadata = 4;
}

// Session
message SessionResponse {
    string session_id = 1;
    optional string video_id = 6;
    optional VideoMetadata video_metadata = 7;
}
```

**Advantages:**
- High performance (binary serialization)
- Type safety (schema validation)
- Bidirectional streaming
- Language agnostic

---

### MCP (Backend â†” Agents)

**Protocol:** Model Context Protocol (stdio)  
**Format:** JSON-RPC over stdin/stdout

**Tool Call:**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "extract_frames",
    "arguments": {
      "video_path": "data/sessions/{id}/uploads/video.mp4",
      "interval_seconds": 30,
      "output_dir": "data/sessions/{id}/temp"
    }
  }
}
```

**Tool Response:**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "success": true,
    "frames": [
      {"frame_index": 0, "timestamp": 0.0, "path": "...frame_0.jpg"},
      {"frame_index": 1, "timestamp": 30.0, "path": "...frame_1.jpg"}
    ]
  }
}
```

**Advantages:**
- Standardized agent communication
- Process isolation (agents crash independently)
- Easy debugging (JSON messages)
- Extensible (add new tools easily)

---

### Ollama API (Backend â†” LLM)

**Protocol:** HTTP REST  
**Port:** 11434  
**Format:** JSON

**Chat Completion:**
```python
response = ollama.chat(
    model='llama3.2',
    messages=[
        {'role': 'system', 'content': 'You are a video analysis assistant.'},
        {'role': 'user', 'content': 'Summarize this video based on: ...'}
    ]
)
```

**Vision Analysis:**
```python
response = ollama.chat(
    model='llava',
    messages=[{
        'role': 'user',
        'content': 'Describe this image concisely',
        'images': [base64_encoded_image]
    }]
)
```

---

## Data Structures

### Video Metadata
```python
@dataclass
class VideoInfo:
    id: str
    filename: str
    file_path: str
    duration: float          # seconds
    resolution: str          # "1920x1080"
    fps: float
    file_size: int           # bytes
    upload_time: datetime
```

### Session Data
```json
{
  "session_id": "uuid",
  "created_at": "2025-10-19T10:00:00Z",
  "last_active": "2025-10-19T10:30:00Z",
  "video_id": "uuid",
  "video_filename": "example.mp4",
  "video_metadata": {
    "duration_ms": 150000,
    "width": 1920,
    "height": 1080,
    "fps": 30.0,
    "file_size": 52428800
  },
  "message_count": 15,
  "analysis_complete": true
}
```

### Cache Entry
```json
{
  "transcription": {
    "text": "Full transcription text...",
    "timestamps": [...],
    "generated_at": "2025-10-19T10:15:00Z"
  },
  "visual_analysis": {
    "frame_analyses": [
      {
        "frame": 0,
        "timestamp": 0.0,
        "description": "A person coding on a laptop",
        "objects": ["person", "laptop", "desk"],
        "text": "console.log('Hello');"
      }
    ],
    "generated_at": "2025-10-19T10:20:00Z"
  }
}
```

---

## Cache Strategy

### Cache-First Philosophy

**Benefits:**
1. **Speed:** Instant responses for repeated queries
2. **Cost:** No redundant processing
3. **Offline:** Works without internet after initial analysis
4. **Consistency:** Same data across requests

**Cache Locations:**
```
data/sessions/{session_id}/cache/
â”œâ”€â”€ transcription.json      # Audio-to-text
â”œâ”€â”€ frames.json             # Extracted frames metadata
â”œâ”€â”€ visual_analysis.json    # Frame descriptions
â”œâ”€â”€ objects.json            # Detected objects
â”œâ”€â”€ text.json               # OCR results
â”œâ”€â”€ summary.json            # Generated summaries
â”œâ”€â”€ pdf_report.json         # PDF metadata
â””â”€â”€ ppt_report.json         # PPT metadata
```

**Cache Invalidation:**
- Automatic: When video changes
- Manual: User can clear cache
- TTL: Optional time-based expiry (future)

**Cache Hit Example:**
```
User: "What's in this video?"
â”œâ”€ Check cache: summary.json âœ… EXISTS
â””â”€ Return: Cached summary (< 10ms)

User: "Create a PDF report"
â”œâ”€ Check cache: summary.json âœ… EXISTS
â”œâ”€ Check cache: transcription.json âœ… EXISTS
â”œâ”€ Check cache: visual_analysis.json âœ… EXISTS
â””â”€ Generate: PDF using cached data (< 1s)
```

---

## Error Handling

### Graceful Degradation

```python
try:
    # Try primary method
    result = await agent.execute_tool(...)
except AgentTimeoutError:
    # Fallback to cache or default
    result = cache.get("fallback") or default_result
except AgentCrashError:
    # Log and notify user
    logger.error("Agent crashed")
    return {"error": "Analysis temporarily unavailable"}
```

### User-Facing Errors

**Types:**
- `VIDEO_NOT_FOUND` - Video file missing
- `AGENT_TIMEOUT` - Agent took too long
- `ANALYSIS_FAILED` - Processing error
- `CACHE_CORRUPT` - Cache data invalid

**Example:**
```typescript
if (response.error) {
  toast.error(`Analysis failed: ${response.error}`);
  // Show retry button
}
```

---

## Performance Characteristics

| Operation | Speed | Bottleneck |
|-----------|-------|------------|
| Video Upload | 10-50 MB/s | Disk I/O |
| Chat Response | 50-200 ms | Ollama inference |
| Frame Extraction | 5-20 fps | CPU/Disk |
| Transcription | 1-5x realtime | Whisper model |
| Vision Analysis | 0.5-2 fps | LLaVA model |
| PDF Generation | < 1s | Content size |
| Cache Hit | < 10ms | Disk read |

---

## Security & Privacy

### Local-First Architecture
- âœ… All processing on-device
- âœ… No cloud API calls
- âœ… No telemetry
- âœ… User data never leaves machine

### File Security
- Videos stored in configured directory
- Temporary files cleaned after processing
- File size limits enforced
- Path traversal protection

### Desktop Sandboxing
- Tauri provides OS-level sandboxing
- Limited file system access
- No arbitrary code execution
- Signed binaries in production

---

## Future Enhancements

### Performance
- [ ] Parallel agent execution
- [ ] GPU acceleration for vision
- [ ] Persistent video database (SQLite)
- [ ] Smart cache preloading

### Features
- [ ] Multi-video comparison
- [ ] Custom report templates
- [ ] Export to more formats (JSON, CSV)
- [ ] Video timeline scrubbing

### Architecture
- [ ] Plugin system for custom agents
- [ ] Distributed processing (multiple machines)
- [ ] Real-time collaboration
- [ ] Cloud sync (optional)

---

**For setup instructions, see:** [QUICKSTART.md](../QUICKSTART.md)  
**For user guide, see:** [README.md](../README.md)
